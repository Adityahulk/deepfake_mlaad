{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Deepfake Audio Detection - Colab Test\n",
                "\n",
                "**IMPORTANT**: Run Cell 1, then **RESTART RUNTIME** before continuing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 1: Install dependencies (RUN ONCE, THEN RESTART RUNTIME)\n",
                "!pip uninstall datasets -y -q 2>/dev/null\n",
                "!pip install datasets==2.14.7 soundfile librosa -q\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"âœ… INSTALLATION COMPLETE\")\n",
                "print(\"ðŸ‘‰ NOW GO TO: Runtime â†’ Restart runtime\")\n",
                "print(\"ðŸ‘‰ THEN skip this cell and run Cell 2\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 2: Upload model file\n",
                "from google.colab import files\n",
                "print(\"ðŸ“¤ Upload your model file (prosody_encoder_best_yet.pth):\")\n",
                "uploaded = files.upload()\n",
                "MODEL_PATH = list(uploaded.keys())[0]\n",
                "print(f\"âœ… Uploaded: {MODEL_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 3: All imports and config\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import torchaudio.transforms as T\n",
                "import numpy as np\n",
                "from sklearn.metrics import roc_curve\n",
                "from datasets import load_dataset\n",
                "from tqdm.auto import tqdm\n",
                "import gc\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "SAMPLE_RATE = 16000\n",
                "N_MELS = 80\n",
                "MAX_AUDIO_LEN_SECONDS = 4\n",
                "MAX_LEN_SAMPLES = SAMPLE_RATE * MAX_AUDIO_LEN_SECONDS\n",
                "MAX_MEL_FRAMES = int(MAX_AUDIO_LEN_SECONDS * (SAMPLE_RATE / 160)) + 1\n",
                "\n",
                "print(f\"Device: {DEVICE}\")\n",
                "print(f\"Max audio samples: {MAX_LEN_SAMPLES}\")\n",
                "print(f\"Max mel frames: {MAX_MEL_FRAMES}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 4: Model definition (EXACT copy from test.py)\n",
                "class ResBlock(nn.Module):\n",
                "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
                "        super(ResBlock, self).__init__()\n",
                "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
                "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
                "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, 1, padding)\n",
                "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
                "        self.shortcut = nn.Sequential()\n",
                "        if stride != 1 or in_channels != out_channels:\n",
                "            self.shortcut = nn.Sequential(\n",
                "                nn.Conv2d(in_channels, out_channels, 1, stride),\n",
                "                nn.BatchNorm2d(out_channels)\n",
                "            )\n",
                "\n",
                "    def forward(self, x):\n",
                "        out = F.relu(self.bn1(self.conv1(x)))\n",
                "        out = self.bn2(self.conv2(out))\n",
                "        out += self.shortcut(x)\n",
                "        out = F.relu(out)\n",
                "        return out\n",
                "\n",
                "class ProsodyEncoder(nn.Module):\n",
                "    def __init__(self, n_mels=N_MELS, num_features=256):\n",
                "        super(ProsodyEncoder, self).__init__()\n",
                "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
                "        self.bn1 = nn.BatchNorm2d(64)\n",
                "        self.layer1 = ResBlock(64, 64, stride=(1,2))\n",
                "        self.layer2 = ResBlock(64, 128, stride=(2,2)) \n",
                "        self.layer3 = ResBlock(128, 256, stride=(2,2))\n",
                "        self.layer4 = ResBlock(256, 512, stride=(2,2))\n",
                "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
                "        self.fc1 = nn.Linear(512, num_features)\n",
                "        self.fc2 = nn.Linear(num_features, 2) \n",
                "\n",
                "    def forward(self, x):\n",
                "        x = F.relu(self.bn1(self.conv1(x)))\n",
                "        x = self.layer1(x)\n",
                "        x = self.layer2(x)\n",
                "        x = self.layer3(x)\n",
                "        x = self.layer4(x)\n",
                "        x = self.pool(x)\n",
                "        x = x.view(x.size(0), -1)\n",
                "        features = F.relu(self.fc1(x))\n",
                "        out_spoof = self.fc2(features)\n",
                "        return features, out_spoof\n",
                "\n",
                "print(\"âœ… Model class defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 5: Load model weights\n",
                "model = ProsodyEncoder().to(DEVICE)\n",
                "state_dict = torch.load(MODEL_PATH, map_location=DEVICE, weights_only=True)\n",
                "model.load_state_dict(state_dict)\n",
                "model.eval()\n",
                "print(\"âœ… Model weights loaded\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 6: Load HuggingFace dataset\n",
                "print(\"ðŸ“¥ Loading ASVspoof 2019 LA eval dataset...\")\n",
                "hf_dataset = load_dataset(\"Bisher/ASVspoof_2019_LA\", split=\"eval\", streaming=False)\n",
                "print(f\"âœ… Loaded {len(hf_dataset)} samples\")\n",
                "\n",
                "# Check first sample to understand structure\n",
                "sample = hf_dataset[0]\n",
                "print(f\"\\nSample keys: {sample.keys()}\")\n",
                "print(f\"Label key name: 'key', value: {sample['key']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 7: Create PyTorch Dataset wrapper\n",
                "class HFASVspoofDataset(Dataset):\n",
                "    def __init__(self, hf_ds):\n",
                "        self.hf_ds = hf_ds\n",
                "        self.mel_transform = T.MelSpectrogram(\n",
                "            sample_rate=SAMPLE_RATE, n_fft=400, win_length=400, hop_length=160, n_mels=N_MELS\n",
                "        )\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.hf_ds)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        item = self.hf_ds[idx]\n",
                "        \n",
                "        # Get audio\n",
                "        audio_array = item['audio']['array']\n",
                "        sr = item['audio']['sampling_rate']\n",
                "        \n",
                "        # Convert to tensor\n",
                "        waveform = torch.FloatTensor(audio_array)\n",
                "        \n",
                "        # Resample if needed\n",
                "        if sr != SAMPLE_RATE:\n",
                "            resampler = T.Resample(sr, SAMPLE_RATE)\n",
                "            waveform = resampler(waveform)\n",
                "        \n",
                "        # Pad/Truncate to fixed length\n",
                "        if waveform.shape[0] > MAX_LEN_SAMPLES:\n",
                "            waveform = waveform[:MAX_LEN_SAMPLES]\n",
                "        else:\n",
                "            padding = torch.zeros(MAX_LEN_SAMPLES - waveform.shape[0])\n",
                "            waveform = torch.cat((waveform, padding), dim=0)\n",
                "        \n",
                "        # Create mel spectrogram\n",
                "        melspec = self.mel_transform(waveform)\n",
                "        melspec = melspec.unsqueeze(0)  # Add channel dim: (1, N_MELS, TIME)\n",
                "        \n",
                "        # Pad/Truncate mel frames\n",
                "        if melspec.shape[2] > MAX_MEL_FRAMES:\n",
                "            melspec = melspec[:, :, :MAX_MEL_FRAMES]\n",
                "        else:\n",
                "            pad = torch.zeros(1, N_MELS, MAX_MEL_FRAMES - melspec.shape[2])\n",
                "            melspec = torch.cat((melspec, pad), dim=2)\n",
                "        \n",
                "        # Label: key is integer 0 or 1 in this dataset\n",
                "        # 0 = bonafide, 1 = spoof\n",
                "        label = int(item['key'])\n",
                "        \n",
                "        return melspec, label\n",
                "\n",
                "# Test the dataset\n",
                "test_ds = HFASVspoofDataset(hf_dataset)\n",
                "test_mel, test_label = test_ds[0]\n",
                "print(f\"âœ… Dataset wrapper works!\")\n",
                "print(f\"   Mel shape: {test_mel.shape}\")\n",
                "print(f\"   Label: {test_label}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 8: Run evaluation (low memory version)\n",
                "def calculate_eer(labels, scores):\n",
                "    fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=1)\n",
                "    fnr = 1 - tpr\n",
                "    eer_index = np.nanargmin(np.abs(fpr - fnr))\n",
                "    return (fpr[eer_index] + fnr[eer_index]) / 2\n",
                "\n",
                "print(f\"\\nðŸš€ Starting evaluation on {len(test_ds)} samples...\")\n",
                "\n",
                "all_scores = []\n",
                "all_labels = []\n",
                "BATCH_SIZE = 32\n",
                "\n",
                "# Process in batches manually to save memory\n",
                "with torch.no_grad():\n",
                "    for start_idx in tqdm(range(0, len(test_ds), BATCH_SIZE)):\n",
                "        end_idx = min(start_idx + BATCH_SIZE, len(test_ds))\n",
                "        \n",
                "        batch_mels = []\n",
                "        batch_labels = []\n",
                "        \n",
                "        for idx in range(start_idx, end_idx):\n",
                "            mel, label = test_ds[idx]\n",
                "            batch_mels.append(mel)\n",
                "            batch_labels.append(label)\n",
                "        \n",
                "        # Stack into batch tensor\n",
                "        batch_tensor = torch.stack(batch_mels).to(DEVICE)\n",
                "        \n",
                "        # Forward pass\n",
                "        _, out_spoof = model(batch_tensor)\n",
                "        \n",
                "        # Get probabilities for class 1 (spoof)\n",
                "        probs = F.softmax(out_spoof, dim=1)[:, 1]\n",
                "        \n",
                "        all_scores.extend(probs.cpu().numpy())\n",
                "        all_labels.extend(batch_labels)\n",
                "        \n",
                "        # Free memory\n",
                "        del batch_tensor, batch_mels\n",
                "        if start_idx % 1000 == 0:\n",
                "            gc.collect()\n",
                "            torch.cuda.empty_cache()\n",
                "\n",
                "print(f\"\\nâœ… Processed {len(all_scores)} samples\")\n",
                "print(f\"   Bonafide (0): {all_labels.count(0)}\")\n",
                "print(f\"   Spoof (1): {all_labels.count(1)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 9: Calculate and display EER\n",
                "eer = calculate_eer(all_labels, all_scores)\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(f\"ðŸŽ¯ ASVspoof 2019 LA Eval EER: {eer * 100:.2f}%\")\n",
                "print(\"=\"*50)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}